{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reffie Take-Home Interview:  Building smart reply feature\n",
    "\n",
    "Trung Le\n",
    "\n",
    "May 8, 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import necessary libaries\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts.chat import ChatMessage\n",
    "from langchain.memory import ChatMessageHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to implement smart reply feature using LLMs, I use the following external libraries:\n",
    "- `ollama` for running LLMs locally\n",
    "- `langchain` modularizing LLM-building workflow and support for multiple LLMs\n",
    "- `pydantic` for coercing LLM output into a structured format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the `pydantic` model\n",
    "\n",
    "Smart Reply feature requires exactly three reply suggestions. Therefore, I construct a Pydantic model `SmartReplies` to faciliate structuring and validation of LLMs output to a class of which instance includes exactly three fields. This Pydantic model allows easy validation and serialization of data, serving as a key component for the frontend.\n",
    "\n",
    "`PydanticOutputParser` will parse the LLM output to an instance of the `SmartReplies` or raise a `ValidationError` if the output cannot form a valid model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Pydantic class\n",
    "class SmartReplies(BaseModel):\n",
    "    reply_1: str = Field(description = \"Smart Reply 1\")\n",
    "    reply_2: str = Field(description = \"Smart Reply 2\")\n",
    "    reply_3: str = Field(description =\" Smart Reply 3\")\n",
    "\n",
    "        \n",
    "parser = PydanticOutputParser(pydantic_object=SmartReplies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parser will create instruction prompt will can be passed to the LLM to guide the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"reply_1\": {\"title\": \"Reply 1\", \"description\": \"Smart Reply 1\", \"type\": \"string\"}, \"reply_2\": {\"title\": \"Reply 2\", \"description\": \"Smart Reply 2\", \"type\": \"string\"}, \"reply_3\": {\"title\": \"Reply 3\", \"description\": \" Smart Reply 3\", \"type\": \"string\"}}, \"required\": [\"reply_1\", \"reply_2\", \"reply_3\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building our LLM workflow using `langchain`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`langchain` provides a framework that makes it easy to build LLM application. One of the strengths of using `langchain` is the use of chains, i.e. sequencing multiple components together. Here I use a simple chain: prompt + model + output parser.\n",
    "\n",
    "#### Prompt\n",
    "\n",
    "\n",
    "\n",
    "#### Model\n",
    "\n",
    "For the model, I use a Llama3 as the LLM of choice, since it is the best open-sourced LLMs on the market right now, with Ollama simplifying running LLM on local machine. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define template\n",
    "\n",
    "model = ChatOllama(model = 'llama3', temperature=0.5)\n",
    "\n",
    "system_prompt = \"\"\"You are a helpful assistant to the responder. Your role is to suggest EXACTLY 3 distinct responses for the responder in a conversation with the human.\n",
    "\n",
    "Each suggestions contains fewer than {num_words}. Each suggestions represents what the responder would most likely send to the other person, based on the conversation history.\n",
    "\n",
    "If the human is requesting truths, please suggest truthfully. If the human is asking open-ended questions, please suggest creatively.\n",
    "\n",
    "The conversation history is below: \\n{chat_history}\\n. Follow this format:\\n {format_instruction}. Do not include \"properties\" from schema.\n",
    "\"\"\"\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\",  \"{message}\")\n",
    "])\n",
    "\n",
    "\n",
    "chain = template | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add memory retrieval\n",
    "\n",
    "def parse_history(chat_history: ChatMessageHistory) -> str:\n",
    "    result = []\n",
    "    for message in chat_history.messages:\n",
    "        if hasattr(message, 'type'):\n",
    "            if message.type == 'human':\n",
    "                result.append(f\"human: {message.content}\")\n",
    "            elif message.type == 'ai':\n",
    "                result.append(f\"ai: {message.content}\")\n",
    "            elif message.type == 'chat':\n",
    "                 result.append(f\"{message.role}: {message.content}\")\n",
    "    return '\\n'.join(result)\n",
    "\n",
    "\n",
    "def get_replies(message: str, chat_history: ChatMessageHistory, save_to_history: bool = False) -> SmartReplies:\n",
    "    response = chain.invoke({\"message\": message,\n",
    "                  \"num_words\": 10,\n",
    "                  \"format_instruction\": parser.get_format_instructions(),\n",
    "                  \"chat_history\": parse_history(chat_history)})\n",
    "    if save_to_history:\n",
    "        chat_history.add_user_message(message)\n",
    "    return response\n",
    "\n",
    "def reply(message: str, chat_history: ChatMessageHistory):\n",
    "    chat_history.add_message(ChatMessage(role = \"responder\", content = message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SmartReplies(reply_1=\"I'm happy to help! The current time is...\", reply_2=\"Let me check that for you... It's currently...\", reply_3=\"Time request received! That's...\")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test run\n",
    "\n",
    "memory_1 = ChatMessageHistory()\n",
    "\n",
    "get_replies(message=\"Hi. Can I see what time it is?\", chat_history=memory_1, save_to_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply(message=\"It's 9AM\", chat_history=memory_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SmartReplies(reply_1=\"It's morning, specifically 9AM.\", reply_2=\"Morning time! It's 9 o'clock.\", reply_3='Good morning! The current time is 9AM.')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_replies(message=\"So is it nighttime or morning time?\", chat_history=memory_1, save_to_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply(message=\"Morning time\", chat_history=memory_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SmartReplies(reply_1='09:00', reply_2=\"It's morning time!\", reply_3='Current time is 09:00')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_replies(message=\"What is current time in 24hr format?\", chat_history=memory_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
