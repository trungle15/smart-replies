{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reffie Take-Home Interview:  Building smart reply feature\n",
    "\n",
    "Trung Le\n",
    "\n",
    "May 8, 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import necessary libaries\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts.chat import ChatMessage\n",
    "from langchain.memory import ChatMessageHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to implement smart reply feature using LLMs, I use the following external libraries:\n",
    "- `ollama` for running LLMs locally\n",
    "- `langchain` modularizing LLM-building workflow and support for multiple LLMs\n",
    "- `pydantic` for coercing LLM output into a structured format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the `pydantic` model\n",
    "\n",
    "Smart Reply feature requires exactly three reply suggestions. Therefore, I construct a Pydantic model `SmartReplies` to faciliate structuring and validation of LLMs output to a class of which instance includes exactly three fields. This Pydantic model allows easy validation and serialization of data, serving as a key component for the frontend.\n",
    "\n",
    "`PydanticOutputParser` will parse the LLM output to an instance of the `SmartReplies` or raise a `ValidationError` if the output cannot form a valid model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Pydantic class\n",
    "class SmartReplies(BaseModel):\n",
    "    reply_1: str = Field(description = \"Smart Reply 1\")\n",
    "    reply_2: str = Field(description = \"Smart Reply 2\")\n",
    "    reply_3: str = Field(description =\" Smart Reply 3\")\n",
    "\n",
    "        \n",
    "parser = PydanticOutputParser(pydantic_object=SmartReplies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parser will create instruction prompt will can be passed to the LLM to guide the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"reply_1\": {\"title\": \"Reply 1\", \"description\": \"Smart Reply 1\", \"type\": \"string\"}, \"reply_2\": {\"title\": \"Reply 2\", \"description\": \"Smart Reply 2\", \"type\": \"string\"}, \"reply_3\": {\"title\": \"Reply 3\", \"description\": \" Smart Reply 3\", \"type\": \"string\"}}, \"required\": [\"reply_1\", \"reply_2\", \"reply_3\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building our LLM workflow using `langchain`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`langchain` provides a framework that makes it easy to build LLM application. One of the strengths of using `langchain` is the use of chains, i.e. sequencing multiple components together. Here I use a simple chain: prompt + model + output parser.\n",
    "\n",
    "#### Prompt\n",
    "\n",
    "Since this is a conversation between two humans, with the LLM being the conversation suggesting assistant, I construct the prompt such that it recognize the 3rd-person role. I specifically ask the model to generate 3 distinct suggestions, potentially semantically, based on the conversation history. I allow for a variable `num_words` to control the number of words per reply. Then I inject the formatting instructions from the parser.\n",
    "\n",
    "#### Model\n",
    "\n",
    "For the model, I use a Llama3 as the LLM of choice, since it is the best open-sourced LLMs on the market right now, with Ollama simplifying running LLM on local machine. I did not use propietary models since it costs money to use their APIs, and for a simple POC, there is no need to use advanced propietary models. I also tested with Google's Gemma model, but Llama3 outputs are better.\n",
    "\n",
    "I specifically use the chat/instruct version of the LLMs and calling `ChatOllama` since it supports conversations as opposed to simple text output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ChatOllama model\n",
    "model = ChatOllama(model = 'llama3', temperature=0.5)\n",
    "\n",
    "# Define system prompt\n",
    "system_prompt = \"\"\"You are a helpful assistant to the responder. Your role is to suggest EXACTLY 3 distinct responses for the responder in a conversation with the human.\n",
    "\n",
    "Each suggestions contains EXACTLY fewer than {num_words}. Each suggestions represents what the responder would most likely send to the other person, based on the conversation history. Please reply like a human would, be conversational.\n",
    "\n",
    "If appropriate, each suggestion should not be similar semantically. For example, if suggesting time to meet. Your suggestions should be: yes, no, maybe.\n",
    "\n",
    "If the human is requesting truths, please suggest truthfully. If the human is asking open-ended questions, please suggest creatively.\n",
    "\n",
    "The conversation history is below: \\n{chat_history}\\n. Follow this format:\\n {format_instruction}. Do not include \"properties\" from schema.\n",
    "\"\"\"\n",
    "\n",
    "# Construct template, in which the model will be fed with this template and the input message from human\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\",  \"{message}\")\n",
    "])\n",
    "\n",
    "# Construct the chain\n",
    "chain = template | model | parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding conversational memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make our smart replies more tailored to the conversation, it is important that the LLMs retain history of the conversation. Here I use `langchain`'s `ChatMessageHistory`, which is a wrapper class that saves conversations between different entities (i.e. Human and AI).\n",
    "\n",
    "As noted earlier, since the model takes a 3rd-person viewpoint, it is important to distinguish what is the role of each message (i.e. from the human, from the responder or from the AI itself). While `langchain` supports injecting `ChatMessageHistory` into the chain, Ollama model does not support chat history of roles other than AI, Human and System. Therefore I inject the history into system message, as guided by the prompt above. \n",
    "\n",
    "I define a function `parse_history` that takes `ChatMessageHistory` instance and produce a string that represents the history between the human and the responder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_history(chat_history: ChatMessageHistory) -> str:\n",
    "    result = []\n",
    "    for message in chat_history.messages:\n",
    "        if hasattr(message, 'type'):\n",
    "            if message.type == 'human':\n",
    "                result.append(f\"human: {message.content}\")\n",
    "            elif message.type == 'ai':\n",
    "                result.append(f\"ai: {message.content}\")\n",
    "            elif message.type == 'chat':\n",
    "                 result.append(f\"{message.role}: {message.content}\")\n",
    "    return '\\n'.join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now are ready to run a conversation with the help of Smart Replies! \n",
    "\n",
    "To faciliate our conversation. I add two helper functions `get_replies` and `reply`, assuming the role of the responder. \n",
    "\n",
    "- `get_replies`: Takes the message of the human and the conversation history to produce Smart Replies. I limit the suggestion to `7` words per response. Although this may not always be accurate, due to the LLM. \n",
    "- `reply`: Register the reply from the responder to the conversation history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_suggestions(message: str, chat_history: ChatMessageHistory, save_to_history: bool = False) -> SmartReplies:\n",
    "    response = chain.invoke({\"message\": message,\n",
    "                  \"num_words\": 7,\n",
    "                  \"format_instruction\": parser.get_format_instructions(),\n",
    "                  \"chat_history\": parse_history(chat_history)})\n",
    "    if save_to_history:\n",
    "        chat_history.add_user_message(message)\n",
    "    return response\n",
    "\n",
    "def reply(message: str, chat_history: ChatMessageHistory):\n",
    "    chat_history.add_message(ChatMessage(role = \"responder\", content = message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example conversation!\n",
    "\n",
    "In this example, I have a conversation with a human (which is also myself!) about checking the time and coordinating to get some food. This example demonstrates that the AI can remember the history and can suggest diverse response options for the responder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SmartReplies(reply_1=\"Let me check! It's currently 10:45 AM.\", reply_2='I can do that for you! The current time is 10:45 AM.', reply_3=\"Ah, sure thing! As of now, it's 10:45 AM.\")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test run\n",
    "\n",
    "memory_1 = ChatMessageHistory()\n",
    "\n",
    "get_suggestions(message=\"Hi. Can I see what time it is?\", chat_history=memory_1, save_to_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply(message=\"It's 9:35PM\", chat_history=memory_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SmartReplies(reply_1=\"It's night time, isn't it?\", reply_2='Not yet, still evening', reply_3='Yeah, the sun has set')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_suggestions(message=\"So is it nighttime or morning time?\", chat_history=memory_1, save_to_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply(message=\"Nighttime\", chat_history=memory_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SmartReplies(reply_1='9:35 PM', reply_2='0935 hours', reply_3='21:35')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_suggestions(message=\"What is current time in 24hr format?\", chat_history=memory_1, save_to_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply(message=\"It's 21:35\", chat_history=memory_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SmartReplies(reply_1='Sounds good to me!', reply_2=\"I'm down for something\", reply_3='What did you have in mind?')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_suggestions(message=\"Shall we get some late night food?\", chat_history=memory_1, save_to_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply(message=\"Sounds good to me!\", chat_history=memory_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SmartReplies(reply_1='How about that new cafe that just opened?', reply_2='Maybe the diner down the street has some good late-night options', reply_3='I was thinking more like a convenience store or something')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_suggestions(message=\"Where shall we go?\", chat_history=memory_1, save_to_history=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "This notebook demonstrates a simple application using LLMs to generate smart replies based on the conversation. It utilizes `langchain`, `ollama`, and `pydantic` to build up a flexible LLM chain.\n",
    "\n",
    "To improve the system further and with time, further additions below may make this system better:\n",
    "- *Adding more conversational/company data*: depending on our use case, assuming Reffie is the responder, adding data from previous conversations with customers and sales/marketing playbook can help the model in outputting more tailored response. RAG can be used to empower this.\n",
    "- *Trying out with different types of memory*: Injecting the full conversation history will make the model spend more tokens to process and exhaust the context window `langchain` offer buffer memory and summary memory that I would like to play around more, which may help in limiting tokens processed and enhance model reliability.\n",
    "- *Testing out with different prompts*: The current prompt seems to work decent enough, although refining it could enhance the precision and relevance of responses. One potential pathway is to try to shorten it while keeping the suggestions relevant enough.\n",
    "- *Conditional on whether to suggest or not*: Similar to Gmail, there are cases where the conversation may not need suggestions for replies or is very complex that reply suggestions are not necessary. A conditional may help with this, and it potentially can help save processing costs for unnecessary API calls."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
