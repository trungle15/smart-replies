{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "from typing import Dict\n",
    "\n",
    "model = Ollama(model=\"gemma\")\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "    # You can add custom validation logic easily with Pydantic.\n",
    "    @validator(\"setup\")\n",
    "    def question_ends_with_question_mark(cls, field):\n",
    "        if field[-1] != \"?\":\n",
    "            raise ValueError(\"Badly formed question!\")\n",
    "        return field\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# # And a query intended to prompt a language model to populate the data structure.\n",
    "# prompt_and_model = prompt | model\n",
    "# output = prompt_and_model.invoke({\"query\": \"tell me a joke.\"})\n",
    "# parser.invoke(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import Dict\n",
    "\n",
    "# Define the model\n",
    "model = ChatOllama(model = 'gemma')\n",
    "\n",
    "# Defining the Pydantic class\n",
    "class SmartReplies(BaseModel):\n",
    "    reply_1: str = Field(description = \"Smart Reply 1\")\n",
    "    reply_2: str = Field(description = \"Smart Reply 2\")\n",
    "    reply_3: str = Field(description =\" Smart Reply 3\")\n",
    "\n",
    "        \n",
    "parser = PydanticOutputParser(pydantic_object=SmartReplies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define template\n",
    "\n",
    "from langchain_core.prompts.chat import ChatMessage\n",
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant to the responder. Your role is to suggest exactly {num_reply} distinct responses for the responder, each with fewer than {num_words} words, based on the conversation history with the responder and the human. Please be truthful, succinct and clear. The history is attached here \\n{chat_history}\\n. And follow the formatting instruction: \\n{format_instruction}\",),\n",
    "    (\"human\",  \"{message}\")\n",
    "])\n",
    "\n",
    "model = ChatOllama(model = 'gemma', temperature = 0.6)\n",
    "\n",
    "chain = template | model | parser\n",
    "\n",
    "def parse_history(chat_history: ChatMessageHistory):\n",
    "    result = []\n",
    "    for message in chat_history.messages:\n",
    "        if hasattr(message, 'type'):\n",
    "            if message.type == 'human':\n",
    "                result.append(f\"human: {message.content}\")\n",
    "            elif message.type == 'ai':\n",
    "                result.append(f\"ai: {message.content}\")\n",
    "            elif message.type == 'chat':\n",
    "                 result.append(f\"{message.role}: {message.content}\")\n",
    "    return '\\n'.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add memory retrieval\n",
    "\n",
    "def get_replies(message: str, chat_history: ChatMessageHistory, save_to_history: bool = False) -> SmartReplies:\n",
    "    # if not chat_history:\n",
    "    #     chat_history = ChatMessageHistory()\n",
    "    response = chain.invoke({\"message\": message, \n",
    "                  \"num_reply\": 3,\n",
    "                  \"num_words\": 10,\n",
    "                  \"format_instruction\": parser.get_format_instructions(),\n",
    "                  \"chat_history\": parse_history(chat_history)})\n",
    "    if save_to_history:\n",
    "        chat_history.add_user_message(message)\n",
    "    return response\n",
    "\n",
    "def reply(message: str, chat_history: ChatMessageHistory):\n",
    "    chat_history.add_message(ChatMessage(role = \"responder\", content = message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SmartReplies(reply_1='Current time is 8:15 PM.', reply_2='The time is displayed at the top of the screen.', reply_3='Check the system tray for the time.')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test run\n",
    "\n",
    "memory_1 = ChatMessageHistory()\n",
    "\n",
    "get_replies(message=\"Hi. Can I see what time it is?\", chat_history=memory_1, save_to_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply(message=\"It's 9AM\", chat_history=memory_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SmartReplies(reply_1='Morning!', reply_2='The day has just begun!', reply_3='Time for a productive day!')"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_replies(message=\"So is it nighttime or morning time?\", chat_history=memory_1, save_to_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply(message=\"Morning time\", chat_history=memory_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SmartReplies(reply_1=\"It's 9AM in 24hr format.\", reply_2='The current time is 09:00.', reply_3='Current time is 9:00 AM.')"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_replies(message=\"What is current time in 24hr format?\", chat_history=memory_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "\n",
    "human_prompt = \"Summarize our conversation so far in {word_count} words.\"\n",
    "human_message_template = HumanMessagePromptTemplate.from_template(human_prompt)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [MessagesPlaceholder(variable_name=\"conversation\"), human_message_template]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
